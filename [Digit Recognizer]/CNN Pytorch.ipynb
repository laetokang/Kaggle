{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from skimage.util.shape import view_as_windows, view_as_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.nn.functional : Convolution, Pooling, Non-linear activation, linear, Dropout, embedding, Loss 등과 관련된 function들이 있음.\n",
    "- skimage.util.shape.view_as_windows : Block view of the input n-dimensional array (using re-striding).\n",
    "- skimage.util.shape.view_as_blocks : Rolling window view of the input n-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\BH2\\\\study\\\\Kaggle\\\\Digit Recognizer'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "side = 28\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, side):\n",
    "        self.df = df_train\n",
    "        self.side = side\n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            idx = [idx]\n",
    "        sample_df = self.df.loc[idx]\n",
    "        return {'X': sample_df[sample_df.columns[1:]].values.reshape(1,side,side)/255.0, 'y':sample_df['label'].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageDataset(df_train, side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_set을 보고 싶은데 어떻게 보지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20396517190>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss as a torch.nn.Module subclass with the actual loss evaluation in the forward method\n",
    "# Use torch functions to have the gradients handled by autograd\n",
    "  \n",
    "class categorical_cross_entropy(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self,y_hat,y):  \n",
    "        for i in range(y_hat.shape[0]):\n",
    "            y_b = torch.zeros(y_hat.shape[1])\n",
    "            y_b[y[i]] = 1\n",
    "            left = -(y_b * torch.log(eps+torch.abs(y_hat[i])))\n",
    "            right = - (1-y_b) * torch.log(eps+torch.abs(1-y_hat[i]))\n",
    "            cce =  (left+right).sum()\n",
    "            if cce != cce or cce == np.inf:\n",
    "                print(cce)\n",
    "                print('y_b', y_b.shape,y_b)\n",
    "                print('y_hat',y_hat[i].shape,y_hat[i])\n",
    "                print('left',left)\n",
    "                print('right',right)\n",
    "                raise Exception('Loss become NaN or infinite')\n",
    "            if i == 0:\n",
    "                loss = cce\n",
    "            else:\n",
    "                loss += cce\n",
    "        return loss/(y_hat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define complex function (e.g. complex activation) as a torch.autograd.Function subclass\n",
    "# Define forward and backward methods as static methods\n",
    "# forward() takes the input (e.g. from previous layer activations) \n",
    "# saves necessary context objects, applies the function and returns the output\n",
    "# backward() takes as input the gradient from the more inner function of the gradient chain (e.g. from next layer activations)\n",
    "# uses necessary context objects, applies the gradient of the function and returns the next step of the gradient chain\n",
    "# if any the function will also return the gradient with respect to the layer's parameters (e.g. fully connected layers, convolutional layers)\n",
    "\n",
    "# LeakyReLU(x) = x if x > 0, 0.01x elsewhere\n",
    "# d LeakyRelu(x)/dx = 1 if x > 0, 0.01 elsewhere\n",
    "class LeakyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = input.clone()\n",
    "        output[output<0] = output[output<0]*0.01\n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print('Leaky_Relu fw', (input != input).any())\n",
    "            print('Leaky_Relu fw', (output != output).any())\n",
    "            #raise Exception('Problema in LeakyRelu Forward')\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = grad_input[input < 0]*0.01\n",
    "        if  (grad_input != grad_input).any() or (grad_input == np.inf).any():\n",
    "            print('Leaky_Relu bw', grad_output.sum(), grad_output.shape)\n",
    "            print('Leaky_Relu bw', grad_input.sum())\n",
    "            #raise Exception('Problema in LeakyRelu Backward')\n",
    "        return grad_input    \n",
    "\n",
    "    \n",
    "# Define the complex function as a normal function using apply\n",
    "\n",
    "leaky_relu = LeakyReLU.apply\n",
    "\n",
    "\n",
    "class Sigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = 1/(1+torch.exp(-input))\n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print('sigmoid fw', (input != input).any())\n",
    "            print('sigmoid fw', (output != output).any())\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = 1/(1+torch.exp(-input))\n",
    "        grad_input = grad_output*grad_input * (1-grad_input)\n",
    "        if  (grad_input != grad_input).any() or (grad_input == np.inf).any():\n",
    "            print('sigmoid bw', grad_output.sum(), grad_output.shape)\n",
    "            print('sigmoid bw', grad_input.sum())\n",
    "        return grad_input\n",
    "    \n",
    "sigmoid = Sigmoid.apply\n",
    "\n",
    "    \n",
    "# Define layers as a torch.nn.Module subclass, defines values to be optimised as Parameters\n",
    "# Use torch functions to have the gradients handled by autograd\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.b = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # smart initialisation as per smart researchers' directive\n",
    "        nn.init.kaiming_uniform_(self.W, a=np.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W)\n",
    "        bound = 1 / np.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.b, -bound, bound)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input.mm(self.W.t())+self.b\n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print(output == np.inf)\n",
    "            print('Linear input', input.sum())\n",
    "            print('Linear output', output.sum())\n",
    "            print('Linear weights', self.W.sum())\n",
    "            print('Linear bias', self.b.sum())\n",
    "            #raise Exception('Problema in Linear Forward')\n",
    "        return output\n",
    "        \n",
    "        \n",
    "# Drop activations randomly during training\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            output = torch.where(torch.rand_like(input) < self.p, torch.zeros_like(input), input)\n",
    "        else:\n",
    "            output = input\n",
    "            \n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print('Dropout input', input.sum())\n",
    "            print('Dropout output', output.sum())\n",
    "            #raise Exception('Problema in Dropout Forward')\n",
    "        return output\n",
    "    \n",
    "# Drop entire channels randomly during training\n",
    "    \n",
    "class Dropout2D(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            dropping_channels = torch.where(torch.rand(input.shape[1]) < self.p, torch.zeros(input.shape[1]), torch.ones(input.shape[1])) == 1#torch.randn(input.shape[1]) > 0\n",
    "            output = input.clone()\n",
    "            output[:,dropping_channels,:,:]=0\n",
    "        else:\n",
    "            output = input\n",
    "            \n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print('Dropout2d input', input.sum())\n",
    "            print('Dropout2d output', output.sum())\n",
    "            #raise Exception('Problema in Dropout2d Forward')\n",
    "        return output\n",
    "    \n",
    "\n",
    "    \n",
    "# Define complex layers with custom backward as torch.nn.Module with helper autograd.Function\n",
    "# This is NOT how you do convolution nowadays since it's very slow, look at Winograd fast convolutions\n",
    "    \n",
    "## Use little helper functions too\n",
    "### terrible performance, unused, naive implementation of convolution with loops\n",
    "def naive_convolution(A,B,n_out,k_size): \n",
    "    C = np.zeros((n_out, n_out))\n",
    "    for j in range(n_out):\n",
    "        for k in range(n_out):\n",
    "            C[j,k] = (A[:,j:j+k_size,k:k+k_size]*B).sum() \n",
    "    return C\n",
    "\n",
    "### bad performance, unused, naive implementation of windowing, use matrix multiplication instead of Hadamard product\n",
    "def naive_no_hadamard_conv(A,B_flattened,n_out,k_size):  \n",
    "    A_ = np.zeros((A.shape[0]*k_size*k_size, n_out*n_out))\n",
    "    i = 0\n",
    "    for j in range(n_out):\n",
    "        for k in range(n_out):\n",
    "            A_[:,i] = A[:,j:j+k_size,k:k+k_size].flatten()\n",
    "            i += 1\n",
    "    return B_flattened.dot(A_).reshape(n_out,n_out)\n",
    "\n",
    "### passable performance, other optimisations are possible, changes memory representation of data to window efficiently, then uses matrix multiplication\n",
    "### A has 3 dimensions, B has 4, output has 3\n",
    "def strided_no_hadamard_conv(A,B,B_flattened,n_out,k_size): \n",
    "    return np.dot(B_flattened,view_as_windows(A,B.shape[1:]).reshape(n_out*n_out, B_flattened.shape[1]).T).reshape(B.shape[0], n_out, n_out)\n",
    "\n",
    "def convolute(A, B, B_flattened):\n",
    "    # Prepare A for full convolution (which is a modified convolution), used in gradient computation\n",
    "    if A.shape[1] < B.shape[2]:\n",
    "        A_ = np.zeros((A.shape[0],(B.shape[2]-1)*2+A.shape[1],(B.shape[2]-1)*2+A.shape[1]))\n",
    "        A_[:,B.shape[2]-1:B.shape[2]-1+A.shape[1],B.shape[2]-1:B.shape[2]-1+A.shape[1]] = A\n",
    "        A = A_\n",
    "    \n",
    "    # A is the input, B is the kernel, B_flattened is the kernel manipulated and already stored for better performance\n",
    "    k_size = B.shape[2]\n",
    "    \n",
    "    # Compute the size of the output image, no stride implemented, nor padding\n",
    "    n_out = A.shape[1] - k_size + 1\n",
    "    \n",
    "    #C = naive_convolution(A,B,n_out,k_size) # B was a single filter\n",
    "    #C = naive_no_hadamard_conv(A,B_flattened,n_out,k_size) # B was a single filter\n",
    "    C = strided_no_hadamard_conv(A,B,B_flattened,n_out,k_size) # B is a group of filters\n",
    "    return C\n",
    "    \n",
    "class Convolution2D(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, kernel, stride):\n",
    "        ctx.save_for_backward(input, kernel, stride)\n",
    "        \n",
    "        # Only squared kernel and squared input implemented, strides not working\n",
    "        out_channels = kernel.shape[0]\n",
    "        batch_size = input.shape[0]\n",
    "        in_size = input.shape[2]\n",
    "        n_out = int((in_size - kernel.shape[2])/stride)+1\n",
    "        output = torch.zeros((batch_size, out_channels, n_out, n_out))\n",
    "        input = input.detach().cpu().numpy()\n",
    "        kernel = kernel.detach().cpu().numpy()\n",
    "        kernel_flattened =kernel.reshape(out_channels,-1)\n",
    "        \n",
    "        # for each 3d Tensor element in batch (Channels X Size X Size)\n",
    "        # convolute the element with the kernel (Number of filters X Channels X Kernel Size X Kernel Size)\n",
    "        # to have one 3d Tensor (Number of filters X Output Size x Output Size)\n",
    "        # complete output is a 4d Tensor (Batch Size X Number of filters X Output Size x Output Size)\n",
    "        for batch in range(batch_size):\n",
    "            output[batch] = torch.Tensor(convolute(input[batch], kernel, kernel_flattened))\n",
    "            \n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print('Conv2d input fw', input.sum())\n",
    "            print('Conv2d output fw', output.sum())\n",
    "            print('Conv2d kernel fw', kernel.sum())\n",
    "            #raise Exception('Problema in Conv2d Forward')\n",
    "            \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, kernel, stride = ctx.saved_tensors\n",
    "        \n",
    "        # Compute gradients with respect to the input and gradients with respect to the weights\n",
    "        batch_size = input.shape[0]\n",
    "        out_size = input.shape[2]\n",
    "        grad_kernel = np.zeros((kernel.shape[1], kernel.shape[0],kernel.shape[2],kernel.shape[3]))\n",
    "        grad_input = np.zeros((kernel.shape[1], batch_size, out_size, out_size))\n",
    "        \n",
    "        # The mathematical subtlety is lost to me, go check out gradients of convolutional layers on your preferred search engine\n",
    "        # Just know that convolution can be used to compute both with some neat tricks, among which flipping kernels, swapping axes, doing a full convolution.\n",
    "        \n",
    "        input = np.swapaxes(input.detach().numpy(), 0,1)\n",
    "        flipped_kernel = np.swapaxes(kernel.flip([2,3]).detach().numpy(), 0,1)\n",
    "        grad_output = grad_output.detach().numpy()\n",
    "        grad_output_swap = np.swapaxes(grad_output,0,1)\n",
    "        grad_output_flattened = grad_output.reshape(grad_output.shape[0],-1)\n",
    "        grad_output_swap_flattened = grad_output_swap.reshape(grad_output_swap.shape[0],-1)\n",
    "\n",
    "        for channel_i in range(kernel.shape[1]):\n",
    "            grad_kernel[channel_i] = convolute(input[channel_i], grad_output_swap, grad_output_swap_flattened)\n",
    "            grad_input[channel_i] = convolute(flipped_kernel[channel_i], grad_output, grad_output_flattened)\n",
    "            \n",
    "        grad_input = torch.Tensor(np.swapaxes(grad_input,0,1)).flip([-2,-1])\n",
    "        grad_kernel = torch.Tensor(np.swapaxes(grad_kernel,0,1))\n",
    "            \n",
    "        if (grad_input != grad_input).any() or (grad_kernel != grad_kernel).any() or (grad_input == np.inf).any() or (grad_kernel == np.inf).any():\n",
    "            print('Conv2d input bw', grad_output.sum())\n",
    "            print('Conv2d output bw', grad_input.sum())\n",
    "            print('Conv2d kernel bw', grad_kernel.sum())\n",
    "            #raise Exception('Problema in Conv2d Backward')\n",
    "            \n",
    "        return grad_input, grad_kernel, None\n",
    "\n",
    "convolution2d = Convolution2D.apply\n",
    "    \n",
    "class Conv2DSquared(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.stride = torch.ones(1) # don't want to think about stride in backpropagation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.kernel, a=np.sqrt(5))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input [batch_size x input_channels x rows x columns]\n",
    "        # output [batch_size x output_channels x new rows x new columns]\n",
    "        # kernel [output_channels x input_channels x kernel_size x kernel_size]\n",
    "        return convolution2d(input, self.kernel, self.stride)\n",
    "\n",
    "    \n",
    "class MaxPool2Df(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, pool_size):\n",
    "        pool_t = pool_size * torch.ones(1)\n",
    "        input_array = input.detach().numpy()\n",
    "        \n",
    "        # Compute the mask to be passed to the backward \n",
    "        # the mask is  a tensor of the same shape as the input, with 1 where max value of a pool window is found, 0 elsewhere\n",
    "        mask = np.zeros_like(input_array)\n",
    "        \n",
    "        # Boxed tensors are fast to access views of the pool windows, edits on them affect the original tensor\n",
    "        boxed_input = view_as_blocks(input_array, (input_array.shape[0],input_array.shape[1],pool_size,pool_size))\n",
    "        boxed_input_mat = boxed_input.reshape(-1,pool_size*pool_size)\n",
    "        \n",
    "        # Create mask from the boxed input\n",
    "        boxed_mask_mat = np.zeros(boxed_input_mat.shape)\n",
    "        boxed_mask_mat[np.arange(len(boxed_input_mat)), boxed_input_mat.argmax(axis=1)] = 1\n",
    "        boxed_mask = boxed_mask_mat.reshape(-1,pool_size,pool_size).reshape(boxed_input.shape)\n",
    "        \n",
    "        # Use boxed view to copy the mask values efficiently\n",
    "        mask_blocks = view_as_blocks(mask, (input_array.shape[0],input_array.shape[1],pool_size,pool_size)) \n",
    "        mask_blocks[0] = boxed_mask[0]\n",
    "        \n",
    "        output = np.zeros((input_array.shape[0],input_array.shape[1],int(input_array.shape[2]/pool_size),int(input_array.shape[2]/pool_size)))\n",
    "        output_blocks = view_as_blocks(output, (input_array.shape[0],input_array.shape[1], 1, 1))\n",
    "        \n",
    "        # Find maximum value in each pool window, and use boxed view to copy it into the output\n",
    "        output_blocks[0] = boxed_input_mat.max(axis=1).reshape(output_blocks.shape)[0]\n",
    "        \n",
    "        ctx.save_for_backward(input, torch.Tensor(mask), pool_t)\n",
    "        \n",
    "        output = torch.Tensor(output)\n",
    "        \n",
    "        if (output != output).any() or (output == np.inf).any():\n",
    "            print('MaxPool input fw', input.sum())\n",
    "            print('MaxPool output fw', output.sum())\n",
    "            #raise Exception('Problema in MaxPool Forward')\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, mask, pool_t = ctx.saved_tensors\n",
    "        input_array = input.detach().numpy()\n",
    "        grad_output_array = grad_output.detach().numpy()\n",
    "        pool_size = int(pool_t)\n",
    "        \n",
    "        mask_array = mask.detach().numpy()\n",
    "        grad_input = np.zeros_like(mask_array)\n",
    "        \n",
    "        # Compute gradient with respect to the input by just passing the gradients from the outer layer to \n",
    "        # the neurons that passed the max values in the forward section\n",
    "        \n",
    "        grad_input_blocks = view_as_blocks(grad_input, (mask_array.shape[0],mask_array.shape[1],pool_size,pool_size))\n",
    "        mask_blocks = view_as_blocks(mask_array, (mask_array.shape[0],mask_array.shape[1],pool_size,pool_size)) \n",
    "        grad_output_blocks = view_as_blocks(grad_output_array, (mask_array.shape[0],mask_array.shape[1], 1, 1))\n",
    "        \n",
    "        # Hadamard product between tensor and vector\n",
    "        boxed_grad_input = grad_output_blocks*mask_blocks\n",
    "        grad_input_blocks[0] = boxed_grad_input[0]\n",
    "        \n",
    "        grad_input = torch.Tensor(grad_input)\n",
    "\n",
    "        if (grad_input != grad_input).any() or (grad_input == np.inf).any():\n",
    "            print('MaxPool input bw', grad_output.sum())\n",
    "            print('MaxPool output bw', grad_input.sum())\n",
    "            #raise Exception('Problema in MaxPool Backward')\n",
    "        \n",
    "        return grad_input, None\n",
    "\n",
    "max_pool2d = MaxPool2Df.apply\n",
    "\n",
    "class MaxPool2D(nn.Module):\n",
    "    def __init__(self, pool_size):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return max_pool2d(input, self.pool_size)\n",
    "    \n",
    "    \n",
    "# Define solver/optimiser as a torch.optim.Optimizer subclass\n",
    "\n",
    "class Adamax(optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        \n",
    "        # all required hyperparameters\n",
    "        # lr (learning rate) is the \"step\" size of the descent towards the minimum\n",
    "        # betas are the two constants balancing the novelty of the current gradients with the steadiness of the moments\n",
    "        ## high values favour the moment, low values favour the gradients\n",
    "        # eps is the correction constant to avoid divisions by zero\n",
    "        \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
    "        super(Adamax, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adamax, self).__setstate__(state)            \n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_max'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avg, exp_avg_max = state['exp_avg'], state['exp_avg_max']\n",
    "\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and infinite moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_max = torch.max(exp_avg_max*beta2, np.abs(grad))\n",
    "                state['exp_avg_max'] = exp_avg_max\n",
    "\n",
    "                step_size = group['lr']\n",
    "                p.data.add_((-step_size*exp_avg)/(exp_avg_max+eps))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = Conv2DSquared(1, 16, 3)#nn.Conv2d(1, 32, 3, 1)#\n",
    "        self.conv2 = Conv2DSquared(16, 64, 3)#nn.Conv2d(32, 64, 3, 1)#\n",
    "        self.dropout1 = Dropout2D(0.25)\n",
    "        self.dropout2 = Dropout(0.5)\n",
    "        self.fc1 = Linear(7744, 128) # 7744\n",
    "        self.fc2 = Linear(128, 10)\n",
    "        self.mp1 = MaxPool2D(2)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mp1(x)\n",
    "        x = leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = leaky_relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "base_lr = 1e-6\n",
    "max_lr = 6*1e-3\n",
    "epochs = 10\n",
    "gamma = 0.5\n",
    "step_schedule = 3\n",
    "eps=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\util\\shape.py:246: RuntimeWarning: Cannot provide views on a non-contiguous input array without copying.\n",
      "  warn(RuntimeWarning(\"Cannot provide views on a non-contiguous input \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/33638 (0%)]\tLoss: 6.940490\n",
      "Train Epoch: 0 [6400/33638 (19%)]\tLoss: 1.503161\n",
      "Train Epoch: 0 [12800/33638 (38%)]\tLoss: 1.063951\n",
      "Train Epoch: 0 [19200/33638 (57%)]\tLoss: 0.596476\n",
      "Train Epoch: 0 [25600/33638 (76%)]\tLoss: 0.690907\n",
      "Train Epoch: 0 [32000/33638 (95%)]\tLoss: 0.977625\n",
      "\n",
      "Val Epoch: 0\n",
      " Average validation loss: 1.4459, Accuracy: 7815/8400 (93%)\n",
      "\n",
      "Train Epoch: 1 [0/33638 (0%)]\tLoss: 0.719366\n",
      "Train Epoch: 1 [6400/33638 (19%)]\tLoss: 0.701353\n",
      "Train Epoch: 1 [12800/33638 (38%)]\tLoss: 1.522654\n",
      "Train Epoch: 1 [19200/33638 (57%)]\tLoss: 0.401919\n",
      "Train Epoch: 1 [25600/33638 (76%)]\tLoss: 0.594361\n",
      "Train Epoch: 1 [32000/33638 (95%)]\tLoss: 0.590494\n",
      "\n",
      "Val Epoch: 1\n",
      " Average validation loss: 0.9629, Accuracy: 8008/8400 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/33638 (0%)]\tLoss: 0.414975\n",
      "Train Epoch: 2 [6400/33638 (19%)]\tLoss: 0.462039\n",
      "Train Epoch: 2 [12800/33638 (38%)]\tLoss: 0.603231\n",
      "Train Epoch: 2 [19200/33638 (57%)]\tLoss: 0.284589\n",
      "Train Epoch: 2 [25600/33638 (76%)]\tLoss: 0.416473\n",
      "Train Epoch: 2 [32000/33638 (95%)]\tLoss: 0.457563\n",
      "\n",
      "Val Epoch: 2\n",
      " Average validation loss: 0.8113, Accuracy: 8083/8400 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/33638 (0%)]\tLoss: 0.511492\n",
      "Train Epoch: 3 [6400/33638 (19%)]\tLoss: 0.374009\n",
      "Train Epoch: 3 [12800/33638 (38%)]\tLoss: 0.517858\n",
      "Train Epoch: 3 [19200/33638 (57%)]\tLoss: 0.296497\n",
      "Train Epoch: 3 [25600/33638 (76%)]\tLoss: 0.639490\n",
      "Train Epoch: 3 [32000/33638 (95%)]\tLoss: 0.414593\n",
      "\n",
      "Val Epoch: 3\n",
      " Average validation loss: 0.6912, Accuracy: 8126/8400 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/33638 (0%)]\tLoss: 0.411315\n",
      "Train Epoch: 4 [6400/33638 (19%)]\tLoss: 0.310938\n",
      "Train Epoch: 4 [12800/33638 (38%)]\tLoss: 0.462965\n",
      "Train Epoch: 4 [19200/33638 (57%)]\tLoss: 0.414779\n",
      "Train Epoch: 4 [25600/33638 (76%)]\tLoss: 0.279955\n",
      "Train Epoch: 4 [32000/33638 (95%)]\tLoss: 0.303371\n",
      "\n",
      "Val Epoch: 4\n",
      " Average validation loss: 0.6480, Accuracy: 8145/8400 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/33638 (0%)]\tLoss: 0.245886\n",
      "Train Epoch: 5 [6400/33638 (19%)]\tLoss: 0.274155\n",
      "Train Epoch: 5 [12800/33638 (38%)]\tLoss: 0.464541\n",
      "Train Epoch: 5 [19200/33638 (57%)]\tLoss: 0.181258\n",
      "Train Epoch: 5 [25600/33638 (76%)]\tLoss: 0.384783\n",
      "Train Epoch: 5 [32000/33638 (95%)]\tLoss: 0.291276\n",
      "\n",
      "Val Epoch: 5\n",
      " Average validation loss: 0.5727, Accuracy: 8183/8400 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/33638 (0%)]\tLoss: 0.282345\n",
      "Train Epoch: 6 [6400/33638 (19%)]\tLoss: 0.244242\n",
      "Train Epoch: 6 [12800/33638 (38%)]\tLoss: 0.263662\n",
      "Train Epoch: 6 [19200/33638 (57%)]\tLoss: 0.278105\n",
      "Train Epoch: 6 [25600/33638 (76%)]\tLoss: 0.330448\n",
      "Train Epoch: 6 [32000/33638 (95%)]\tLoss: 0.186710\n",
      "\n",
      "Val Epoch: 6\n",
      " Average validation loss: 0.5414, Accuracy: 8191/8400 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/33638 (0%)]\tLoss: 0.247222\n",
      "Train Epoch: 7 [6400/33638 (19%)]\tLoss: 0.345349\n",
      "Train Epoch: 7 [12800/33638 (38%)]\tLoss: 0.207742\n",
      "Train Epoch: 7 [19200/33638 (57%)]\tLoss: 0.488635\n",
      "Train Epoch: 7 [25600/33638 (76%)]\tLoss: 0.249338\n",
      "Train Epoch: 7 [32000/33638 (95%)]\tLoss: 0.227775\n",
      "\n",
      "Val Epoch: 7\n",
      " Average validation loss: 0.5310, Accuracy: 8206/8400 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/33638 (0%)]\tLoss: 0.132933\n",
      "Train Epoch: 8 [6400/33638 (19%)]\tLoss: 0.272384\n",
      "Train Epoch: 8 [12800/33638 (38%)]\tLoss: 0.233720\n",
      "Train Epoch: 8 [19200/33638 (57%)]\tLoss: 0.220467\n",
      "Train Epoch: 8 [25600/33638 (76%)]\tLoss: 0.254328\n",
      "Train Epoch: 8 [32000/33638 (95%)]\tLoss: 0.292386\n",
      "\n",
      "Val Epoch: 8\n",
      " Average validation loss: 0.5053, Accuracy: 8211/8400 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/33638 (0%)]\tLoss: 0.195315\n",
      "Train Epoch: 9 [6400/33638 (19%)]\tLoss: 0.315942\n",
      "Train Epoch: 9 [12800/33638 (38%)]\tLoss: 0.319640\n",
      "Train Epoch: 9 [19200/33638 (57%)]\tLoss: 0.180621\n",
      "Train Epoch: 9 [25600/33638 (76%)]\tLoss: 0.287484\n",
      "Train Epoch: 9 [32000/33638 (95%)]\tLoss: 0.318873\n",
      "\n",
      "Val Epoch: 9\n",
      " Average validation loss: 0.5045, Accuracy: 8210/8400 (98%)\n",
      "\n",
      "Wall time: 40min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = DataLoader(train_set, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "model = Model().float()\n",
    "\n",
    "# Enveloped sawtooth\n",
    "def current_lr(epoch):\n",
    "    return base_lr + (epochs-(epoch%step_schedule))/epochs*(((gamma**(epoch/epochs)) * max_lr) - base_lr)\n",
    "\n",
    "lr = current_lr(0)\n",
    "\n",
    "optimiser = Adamax(model.parameters(), lr=lr, eps=eps)\n",
    "\n",
    "# Define learning rate scheduler by using torch.optim.lr_scheduler.LambdaLR with a custom function\n",
    "# Function takes current epoch and return the factor by which the original learning rate is multiplied\n",
    "lr_lambda = lambda epoch: current_lr(epoch)/current_lr(0) if epoch > 0 else 1\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimiser, lr_lambda)\n",
    "\n",
    "criterion = categorical_cross_entropy()\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "n = len(train_set)\n",
    "n_batches = len(train)\n",
    "log_interval = 100\n",
    "early_stopping = False\n",
    "\n",
    "# Epoch looping\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Dataset looping, should have divided train and test in two functions and used a different way to divide training and validation set\n",
    "    # But this wasn't my focus on the cookbook so I chose laziness\n",
    "    for i, batch in enumerate(train):\n",
    "        batch_size = batch['y'].shape[0]\n",
    "        X = batch['X'].float()\n",
    "        y = batch['y'].reshape(batch_size).long()\n",
    "        if i < int(n_batches*0.8):\n",
    "            model.train()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            train_loss += loss.item()\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            if i % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, i*batch_size, int(len(train)*batch_size*0.8),100. * i / int(len(train)*0.8), loss.item()))\n",
    "        if i == int(n_batches*0.8):\n",
    "            train_history.append(train_loss/(int(n_batches*0.8)))\n",
    "            scheduler.step()\n",
    "            model.eval()\n",
    "        if i >= int(n_batches*0.8):\n",
    "             with torch.no_grad():\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y)\n",
    "                val_loss += loss.item()\n",
    "                pred = torch.max(y_hat, axis=1)[1]\n",
    "                correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "                total += y.shape[0]\n",
    "    avg_val_loss = val_loss/int(n_batches*0.2)\n",
    "    print('\\nVal Epoch: {}\\n Average validation loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch, val_loss/int(n_batches*0.2), correct, total,100. * correct / total))\n",
    "    val_history.append(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(train_history,label=\"Train Loss\")\n",
    "plt.plot(val_history,label=\"Val Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.Tensor(df_test.values.reshape(-1,1,side,side)/255.0)\n",
    "pred = model(X_test)\n",
    "df_submission['Label'] = pred.argmax(axis=1)\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.subplots(figsize=(10,9))\n",
    "ax = sns.heatmap(conf_matrix, annot=True, vmax=20)\n",
    "ax.set_xlabel('Predicted');\n",
    "ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference :\n",
    "- https://www.kaggle.com/fleanend/homegrown-nn-cookbook-pytorch\n",
    "- https://www.kaggle.com/abhinand05/mnist-introduction-to-computervision-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
