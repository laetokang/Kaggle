{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "print(os.listdir('./data'))\n",
    "file_path ='./data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(file_path + \"/train.csv\")\n",
    "test = pd.read_csv(file_path + \"/test.csv\")\n",
    "submit = pd.read_csv(file_path + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST2(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((28, 28, 1))\n",
    "        label = self.data.iloc[index, 0]\n",
    "        \n",
    "        if self.transform is not None :\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DatasetMNIST2('./data/train.csv', transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(images[1].shape)\n",
    "print(labels[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "img = images[1]\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "npimg = img.numpy()\n",
    "print(npimg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "npimg = np.transpose(npimg, (1, 2, 0))\n",
    "print(npimg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),         # (N, 1, 28, 28) -> (N,  6, 24, 24)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),               # (N, 6, 24, 24) -> (N,  6, 12, 12)\n",
    "            nn.Conv2d(6, 16, 5),                    # (N, 6, 12, 12) -> (N, 16, 8, 8)  \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.MaxPool2d(2, stride=2)                # (N,16, 8, 8) -> (N, 16, 4, 4)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256,120),                     # (N, 256) -> (N, 120)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120,84),                      #  (N, 120) -> (N, 84)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(84,10)                       # (N, 84)  -> (N, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Y_pred = self.forward(X)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(network, criterion, optimizer, epoch_num, test=True):\n",
    "    \n",
    "    print(\"Start Training with\", device, epoch_num, \"overall epoch\")\n",
    "    network.to(device)\n",
    "    \"\"\"\n",
    "    Create Dataloader for training and validating\n",
    "    \"\"\"\n",
    "    composed_transform = transforms.Compose([Regularize(), ToTensor()])\n",
    "    digit_dataset = DigitDataset('train.csv', '../input/', train=True, transform=composed_transform)\n",
    "    if test:\n",
    "        train_indices, val_indices = train_validate_split(digit_dataset.digit_df)\n",
    "        train_sampler = sampler.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = sampler.SubsetRandomSampler(val_indices)\n",
    "        train_dataloader = DataLoader(\n",
    "            digit_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            digit_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            sampler=val_sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        print(\"Training with validation, \", \"Overall Data:\", len(train_indices)+len(val_indices))\n",
    "        print(\"Training Data:\", len(train_indices), \"Validate Data:\", len(val_indices))\n",
    "    else:\n",
    "        train_dataloader = DataLoader(\n",
    "            digit_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_dataloader = None\n",
    "        print(\"Training all data, \", \"Overall Data:\", len(digit_dataset))\n",
    "    \"\"\"\n",
    "    Start Training\n",
    "    \"\"\"\n",
    "    batch_num = 0\n",
    "    ita = []\n",
    "    loss_avg = []\n",
    "    val_acc = []\n",
    "    for epoch in range(epoch_num):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            digits, labels = data\n",
    "            digits, labels = digits.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(digits)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_num += 1\n",
    "            if test == True and i % 500 == 499:\n",
    "                ita.append(batch_num)\n",
    "                loss_avg.append(running_loss/500.)\n",
    "                val_acc.append(validating(network, val_dataloader))\n",
    "                running_loss = 0.\n",
    "    if test:\n",
    "        train_accuracy = validating(network, train_dataloader)\n",
    "        val_accuracy = validating(network, val_dataloader)\n",
    "        print('Training accuracy: %.5f' % (train_accuracy))\n",
    "        print('Validation accuracy: %.5f' % (val_accuracy))\n",
    "    return network, ita, loss_avg, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()\n",
    "out = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0479,  0.0611,  0.1173,  0.0203, -0.0228, -0.0456,  0.1334, -0.0707,\n",
      "         -0.0292,  0.0015],\n",
      "        [-0.0425,  0.0596,  0.1182,  0.0231, -0.0256, -0.0435,  0.1291, -0.0687,\n",
      "         -0.0257, -0.0037],\n",
      "        [-0.0447,  0.0615,  0.1163,  0.0250, -0.0277, -0.0453,  0.1341, -0.0655,\n",
      "         -0.0281, -0.0026],\n",
      "        [-0.0450,  0.0601,  0.1146,  0.0213, -0.0286, -0.0433,  0.1353, -0.0684,\n",
      "         -0.0293, -0.0037]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader):\n",
    "    total, correct = 0, 0\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/16, Train acc: 90.31\n",
      "Epoch: 1/16, Train acc: 93.73\n",
      "Epoch: 2/16, Train acc: 95.99\n",
      "Epoch: 3/16, Train acc: 96.96\n",
      "Epoch: 4/16, Train acc: 97.42\n",
      "Epoch: 5/16, Train acc: 97.91\n",
      "Epoch: 6/16, Train acc: 98.42\n",
      "Epoch: 7/16, Train acc: 98.58\n",
      "Epoch: 8/16, Train acc: 98.93\n",
      "Epoch: 9/16, Train acc: 98.65\n",
      "Epoch: 10/16, Train acc: 98.76\n",
      "Epoch: 11/16, Train acc: 99.10\n",
      "Epoch: 12/16, Train acc: 99.29\n",
      "Epoch: 13/16, Train acc: 99.36\n",
      "Epoch: 14/16, Train acc: 99.28\n",
      "Epoch: 15/16, Train acc: 99.13\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 16\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    print('Epoch: %d/%d, Train acc: %0.2f' % (epoch, max_epochs, evaluation(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = np.loadtxt('./data/test.csv',skiprows=1,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset.reshape(testset.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 28, 28, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = np.transpose(testset, (0, 3, 1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 1, 28, 28)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torch.tensor(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_val = net.predict(testset)\n",
    "pred = torch.argmax(Y_pred_val, dim=1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.to('cpu')\n",
    "results = pd.Series(pred , name = 'Label')\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = 'ImageId'),results],axis=1)\n",
    "submission.to_csv(\"final_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://www.kaggle.com/tauseef6462/cnn-lenet-architecture-using-pytorch\n",
    "- https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "- https://www.kaggle.com/ankschoubey/lenet5-based-cnn-with-98-94-using-pytorch\n",
    "- https://www.kaggle.com/miketonson1006/pytorch-from-a-beginner-s-view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
